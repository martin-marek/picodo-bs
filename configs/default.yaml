defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

seed: 0
batch_size_train: 32
batch_size_valid: 128
num_tokens_train: null
num_tokens_valid: null
eval_every_steps: 100
ds_path_train: null
ds_path_valid: null
wandb_project: 'picodo'
wandb_mode: 'online'

model: # GPT2-small (124M params)
  D: 768  # model/embed dim  = qkv dim
  H: 12  # num attention heads
  L: 1024  # max context/sequence length
  N: 12  # number of transformer block layers
  V: 50257  # vocab size -> must match dataset tokenizer!
  F: 3072  # FF inner dimension
  fsdp_enabled: true
  dtype: null

opt:
  peak_lr: 0.004
  warmup_frac: 0.05
  b1: 0.9
  b2: 0.98
  weight_decay: 0.1
