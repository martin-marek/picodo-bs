defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

seed: 0
ds_path_train: null
ds_path_valid: null
tokens_params_ratio: 20 # chinchilla scaling
num_tokens_train: null
num_tokens_valid: null
batch_size_valid: 128
num_eval_steps: 100
wandb_project: 'picodo-bs'
wandb_mode: 'online'
run_name: null

model:
  k: 2 # model size scaling factor, 6 â‰ˆ 124M param
  D: ${eval:'128 * ${model.k}'} # model/embed/qkv dim
  F: ${eval:'4 * ${model.D}'} # FF inner dimension = 4 x embed dim.
  H: ${eval:'2 * ${model.k}'} # num. attention heads, each with dim. D/H = 64
  L: 512 # context/sequence length
  N: 12 # num. block layers
  V: 50257 # vocab size -> must match dataset tokenizer!
  fsdp_enabled: true
  dtype: null

opt:
  optimizer: 'sgd'
  microbatch_size: 32
  grad_acc_steps: 1
  batch_size: ${eval:'${opt.grad_acc_steps} * ${opt.microbatch_size}'}
  peak_lr: 0.004
  warmup_frac: 0.05
  b1: 0.9
  b2: null
  t2: null
  weight_decay: 0
