defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

seed: 0
ds_path: null
tokens_params_ratio: 20 # chinchilla scaling
num_tokens_train: null
num_tokens_valid: 10_000_000
batch_size_valid: 4
num_eval_steps: 1
pad_eval: false
wandb_project: 'picodo-bs'
wandb_mode: 'online'
run_name: null
num_tp_devices: 1 # optional tensor parallelism

model:
  k: 6
  D: ${mul:128, ${model.k}} # model/embed/qkv dim
  F: ${mul:4, ${model.D}} # FF inner dimension = 4 x embed dim.
  H: ${mul:2, ${model.k}} # num. attention heads, each with dim. D/H = 64
  N: 12 # num. block layers
  L: 512 # context/sequence length
  V: 50257 # vocab size -> must match dataset tokenizer!
  dtype: null

opt:
  optimizer: 'sgd'
  batch_size: 4
  max_microbatch_size: 4
  microbatch_size: ${min:${opt.batch_size}, ${opt.max_microbatch_size}} 
  grad_acc_steps: ${floordiv:${opt.batch_size}, ${opt.microbatch_size}}
  peak_lr: null
  peak_lr_scaled: null
  peak_lr_scaling: null
  warmup_frac: 0.05
  b1: null
  b2: null
  t1: null # units: num. of tokens
  t2: null # units: num. of tokens
  b2_min: null
  weight_decay: 0
